{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script converts combined labels to a dataset that huggingface can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../../lib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import *\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Combinedlabels.csv\n",
    "df = pd.read_csv(\"Combinedlabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tweet_id  Gabe  Guanyi  Mom  Sum  \\\n",
      "0         0     0       0    0    0   \n",
      "1         1     0       0    0    0   \n",
      "2         2     0       0    0    0   \n",
      "3         3     1       1    1    3   \n",
      "4         4     0       0    0    0   \n",
      "\n",
      "                                          tweet_text  \\\n",
      "0  RT {{MENTION}}: Starting in one hour. Raising ...   \n",
      "1  $102.82 this week! This is how I now make a pa...   \n",
      "2  OPEN today 10 am until 1 pm\\n419 CCPkwy Cape C...   \n",
      "3  A superb 18th century European carved ivory di...   \n",
      "4                                 Driving aroundd :P   \n",
      "\n",
      "                                    user_description  \n",
      "0  Can't start a fire... Can't start a fire witho...  \n",
      "1                     Follow me and you'll find out!  \n",
      "2  We are 3rd generation Brocante-Antique Import ...  \n",
      "3  John Nicholson's Auctioneers are the largest a...  \n",
      "4  Im 13 years young\\nim in love with the most am...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in ocr json\n",
    "with open('text_from_images/ids_to_text.json') as f:\n",
    "\tocr_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "492\n"
     ]
    }
   ],
   "source": [
    "# Print length of of data frame\n",
    "print(len(df))\n",
    "\n",
    "# Remove rows with duplicate text\n",
    "df = df.drop_duplicates(subset=['tweet_text'])\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of text:  288\n",
      "Max length of text and user description:  440\n",
      "Max length of text and user description and ocr:  2102\n",
      "Number of tweets:  492\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "texts_userdescriptions = []\n",
    "texts_userdescriptions_ocrs = []\n",
    "labels = []\n",
    "ids = []\n",
    "max_length = 0\n",
    "number_of_tweets = 0\n",
    "for index, row in df.iterrows():\n",
    "\t\tlabel = row['Sum']\n",
    "\t\tif label >= 2:\n",
    "\t\t\tlabels.append(1)\n",
    "\t\telse:\n",
    "\t\t\tlabels.append(0)\n",
    "\t\ttext = analysis.clean_text(row['tweet_text'].replace('\"', '\"\"'))\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tuserdescription = analysis.clean_text(row['user_description'].replace('\"', '\"\"'))\n",
    "\t\texcept:\n",
    "\t\t\tuserdescription = '[nodes]'\n",
    "\t\ttry:\n",
    "\t\t\tocr = analysis.clean_text(ocr_json[str(row['tweet_id'])].replace('\"', '\"\"'))\n",
    "\t\t\t# print(ocr)\n",
    "\t\texcept:\n",
    "\t\t\tocr = '[noocr]'\n",
    "\t\t# Append to each list with adding [sep]\n",
    "\t\ttexts.append(text)\n",
    "\t\ttexts_userdescriptions.append(text + ' [sep] ' + userdescription)\n",
    "\t\ttexts_userdescriptions_ocrs.append(text + ' [sep] ' + userdescription + ' [sep] ' + ocr)\n",
    "\t\tids.append(row['tweet_id'])\n",
    "\t\tnumber_of_tweets += 1\n",
    "  \n",
    "# Get max length for the text in each list\n",
    "max_length_text = max(len(text) for text in texts)\n",
    "max_length_userdescriptions = max(len(text) for text in texts_userdescriptions)\n",
    "max_length_userdescriptions_ocrs = max(len(text) for text in texts_userdescriptions_ocrs)\n",
    "print(\"Max length of text: \", max_length_text)\n",
    "print(\"Max length of text and user description: \", max_length_userdescriptions)\n",
    "print(\"Max length of text and user description and ocr: \", max_length_userdescriptions_ocrs)\n",
    "print(\"Number of tweets: \", number_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 elements of text:  [79, 79, 80, 81, 84, 85, 85, 89, 89, 95]\n",
      "First 10 elements of text and user description:  [111, 113, 113, 114, 114, 116, 118, 121, 127, 137]\n",
      "First 10 elements of text and user description and ocr:  [165, 176, 191, 196, 205, 212, 221, 237, 313, 476]\n"
     ]
    }
   ],
   "source": [
    "# Count number of tokens in each text\n",
    "tokens_in_text = []\n",
    "tokens_in_text_userdescriptions = []\t\n",
    "tokens_in_text_userdescriptions_ocrs = []\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer._add_tokens([\"[nodes]\", \"[noocr]\", \"{{URL}}\", \"{{MENTION}}\", \"{{EMAIL}}\"])\n",
    "for text in texts:\n",
    "\ttokens_in_text.append(len(tokenizer.tokenize(text)))\n",
    "for text in texts_userdescriptions:\n",
    "\ttokens_in_text_userdescriptions.append(len(tokenizer.tokenize(text)))\n",
    "for text in texts_userdescriptions_ocrs:\n",
    "\ttokens_in_text_userdescriptions_ocrs.append(len(tokenizer.tokenize(text)))\n",
    "\n",
    "# Sort the lists\t\n",
    "tokens_in_text = sorted(tokens_in_text)\n",
    "tokens_in_text_userdescriptions = sorted(tokens_in_text_userdescriptions)\n",
    "tokens_in_text_userdescriptions_ocrs = sorted(tokens_in_text_userdescriptions_ocrs)\n",
    "\n",
    "# Print the first 10 elements of each list\n",
    "print(\"First 10 elements of text: \", tokens_in_text[-10:])\n",
    "print(\"First 10 elements of text and user description: \", tokens_in_text_userdescriptions[-10:])\n",
    "print(\"First 10 elements of text and user description and ocr: \", tokens_in_text_userdescriptions_ocrs[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero labels:  315\n",
      "Number of one labels:  177\n"
     ]
    }
   ],
   "source": [
    "# Get the number of zero labels\n",
    "print(\"Number of zero labels: \", len(labels) - sum(labels))\n",
    "print(\"Number of one labels: \", sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good tweets:  315\n",
      "Number of bad tweets:  177\n"
     ]
    }
   ],
   "source": [
    "good_tweets_texts = {}\n",
    "bad_tweets_texts = {}\n",
    "good_tweets_texts_userdescriptions = {}\n",
    "bad_tweets_texts_userdescriptions = {}\n",
    "good_tweets_texts_userdescriptions_ocrs = {}\n",
    "bad_tweets_texts_userdescriptions_ocrs = {}\n",
    "good_tweets_texts[\"tweets\"] = []\n",
    "bad_tweets_texts[\"tweets\"] = []\n",
    "good_tweets_texts_userdescriptions[\"tweets\"] = []\n",
    "bad_tweets_texts_userdescriptions[\"tweets\"] = []\n",
    "good_tweets_texts_userdescriptions_ocrs[\"tweets\"] = []\n",
    "bad_tweets_texts_userdescriptions_ocrs[\"tweets\"] = []\n",
    "\n",
    "for tweet in range(len(texts)):\n",
    "\tout_text = {}\n",
    "\tout_text[\"text\"] = texts[tweet]\n",
    "\tout_text[\"tweet_id\"] = ids[tweet]\n",
    "\tout_text_userdescriptions = {}\n",
    "\tout_text_userdescriptions[\"text\"] = texts_userdescriptions[tweet]\n",
    "\tout_text_userdescriptions[\"tweet_id\"] = ids[tweet]\n",
    "\tout_text_userdescriptions_ocrs = {}\n",
    "\tout_text_userdescriptions_ocrs[\"text\"] = texts_userdescriptions_ocrs[tweet]\n",
    "\tout_text_userdescriptions_ocrs[\"tweet_id\"] = ids[tweet]\n",
    "\n",
    "\tif labels[tweet] == 0:\n",
    "\t\tgood_tweets_texts[\"tweets\"].append(out_text)\n",
    "\t\tgood_tweets_texts_userdescriptions[\"tweets\"].append(out_text_userdescriptions)\n",
    "\t\tgood_tweets_texts_userdescriptions_ocrs[\"tweets\"].append(out_text_userdescriptions_ocrs)\n",
    "\telse:\n",
    "\t\tbad_tweets_texts[\"tweets\"].append(out_text)\n",
    "\t\tbad_tweets_texts_userdescriptions[\"tweets\"].append(out_text_userdescriptions)\n",
    "\t\tbad_tweets_texts_userdescriptions_ocrs[\"tweets\"].append(out_text_userdescriptions_ocrs)\n",
    "  \n",
    "print(\"Number of good tweets: \", len(good_tweets_texts[\"tweets\"]))\n",
    "print(\"Number of bad tweets: \", len(bad_tweets_texts[\"tweets\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to good_tweets.json\n",
    "with open('good_tweets_text.json', 'w') as outfile:\n",
    "\tjson.dump(good_tweets_texts, outfile)\n",
    "\n",
    "with open('bad_tweets_text.json', 'w') as outfile:\n",
    "\tjson.dump(bad_tweets_texts, outfile)\n",
    " \n",
    "with open('good_tweets_text_userdescriptions.json', 'w') as outfile:\n",
    "\tjson.dump(good_tweets_texts_userdescriptions, outfile)\n",
    "\n",
    "with open('bad_tweets_text_userdescriptions.json', 'w') as outfile:\n",
    "\tjson.dump(bad_tweets_texts_userdescriptions, outfile)\n",
    "\n",
    "with open('good_tweets_text_userdescriptions_ocrs.json', 'w') as outfile:\n",
    "\tjson.dump(good_tweets_texts_userdescriptions_ocrs, outfile)\n",
    "\n",
    "with open('bad_tweets_text_userdescriptions_ocrs.json', 'w') as outfile:\n",
    "\tjson.dump(bad_tweets_texts_userdescriptions_ocrs, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
