{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "# max sequence length for each document/sentence sample\n",
    "max_length = 512\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"results/checkpoint-1200\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "test_trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Lemmatize all words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_bad_tweets = {}\n",
    "maybe_bad_tweets[\"tweets\"] = []\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        followers_timeline_file = open(\"../../data/followers_timeline_{}.json\".format(str(i)), 'r')\n",
    "        print(\"Working on file {}\".format(str(i)))\n",
    "    except:\n",
    "        print(\"File not found\")\n",
    "        break\n",
    "    followers_timeline = json.load(followers_timeline_file)\n",
    "    for user in followers_timeline[\"followers_timeline\"]:\n",
    "        for follower in followers_timeline[\"followers_timeline\"][user]:\n",
    "            try:\n",
    "                tweet_timeline = followers_timeline[\"followers_timeline\"][user][follower][\"data\"]\n",
    "                tweet_text_list = []\n",
    "                for tweet in tweet_timeline:\n",
    "                        tweet_text = tweet['text']\n",
    "                        tweet_text = ' '.join([word for word in tweet_text.lower().split() if word not in stop])  # remove stopwords\n",
    "                        # remove punc.\n",
    "                        tweet_text = tweet_text.translate(table)\n",
    "                        tweet_text = ''.join(c for c in tweet_text if not c.isdigit())  # remove numbers\n",
    "                        tweet_text = tweet_text.replace('  ', ' ')  # remove double spaces\n",
    "                        tweet_text = ' '.join([lemmatizer.lemmatize(word) for word in tweet_text.split()])  # lemminize\n",
    "                        tweet_text_list.append(tweet_text)\n",
    "                X_test_tokenized = tokenizer(tweet_text_list, padding=True, truncation=True, max_length=512)\n",
    "                test_dataset = Dataset(X_test_tokenized)\n",
    "                raw_pred, _, _ = test_trainer.predict(test_dataset)\n",
    "                y_pred = np.argmax(raw_pred, axis=1)\n",
    "                counter = 0\n",
    "                for tweet_pred in y_pred:\n",
    "                    if tweet_pred == 1:\n",
    "                        print(tweet_text_list[counter])\n",
    "                        maybe_bad_tweets[\"tweets\"].append(tweet_timeline[counter])\n",
    "                    counter += 1\n",
    "                        \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write maybe bad tweets to file\n",
    "with open(\"maybe_bad_tweets.json\", 'w') as outfile:\n",
    "\tjson.dump(maybe_bad_tweets, outfile)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d259f8ea978a0011a9c41c85f38b722350957b00d06485c4eb037401ff597af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('main': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
