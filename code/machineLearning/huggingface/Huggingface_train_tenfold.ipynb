{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWqACx_OTBuY",
        "outputId": "2a2de81d-0926-4d8e-e2cf-e48851e88e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  6 20:29:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oV79zvYTLOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ae082e-adbc-44b7-b22a-9c86eeeb013b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1M_CHcUa9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a529ca7-23bd-4aa6-ab45-9431581c46ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO7TqvLEUQfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e6f1f5-7f60-4049-f39b-7b5859bad9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WPI/Junior/IQP/Machine Models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python run_glue.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV70Vgx-kIOJ",
        "outputId": "db81df68-a91d-4643-a1af-6fb76716d979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: run_glue.py [-h] --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                   [--config_name CONFIG_NAME]\n",
            "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                   [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
            "                   [--no_use_fast_tokenizer] [--model_revision MODEL_REVISION]\n",
            "                   [--use_auth_token [USE_AUTH_TOKEN]] [--task_name TASK_NAME]\n",
            "                   [--dataset_name DATASET_NAME]\n",
            "                   [--dataset_config_name DATASET_CONFIG_NAME]\n",
            "                   [--max_seq_length MAX_SEQ_LENGTH]\n",
            "                   [--overwrite_cache [OVERWRITE_CACHE]]\n",
            "                   [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n",
            "                   [--no_pad_to_max_length]\n",
            "                   [--max_train_samples MAX_TRAIN_SAMPLES]\n",
            "                   [--max_eval_samples MAX_EVAL_SAMPLES]\n",
            "                   [--max_predict_samples MAX_PREDICT_SAMPLES]\n",
            "                   [--train_file TRAIN_FILE]\n",
            "                   [--validation_file VALIDATION_FILE] [--test_file TEST_FILE]\n",
            "                   --output_dir OUTPUT_DIR\n",
            "                   [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
            "                   [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
            "                   [--do_predict [DO_PREDICT]]\n",
            "                   [--evaluation_strategy {no,steps,epoch}]\n",
            "                   [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
            "                   [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
            "                   [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
            "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                   [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
            "                   [--eval_delay EVAL_DELAY] [--learning_rate LEARNING_RATE]\n",
            "                   [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
            "                   [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
            "                   [--max_grad_norm MAX_GRAD_NORM]\n",
            "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                   [--max_steps MAX_STEPS]\n",
            "                   [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
            "                   [--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
            "                   [--log_level {debug,info,warning,error,critical,passive}]\n",
            "                   [--log_level_replica {debug,info,warning,error,critical,passive}]\n",
            "                   [--log_on_each_node [LOG_ON_EACH_NODE]]\n",
            "                   [--no_log_on_each_node] [--logging_dir LOGGING_DIR]\n",
            "                   [--logging_strategy {no,steps,epoch}]\n",
            "                   [--logging_first_step [LOGGING_FIRST_STEP]]\n",
            "                   [--logging_steps LOGGING_STEPS]\n",
            "                   [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
            "                   [--no_logging_nan_inf_filter]\n",
            "                   [--save_strategy {no,steps,epoch}]\n",
            "                   [--save_steps SAVE_STEPS]\n",
            "                   [--save_total_limit SAVE_TOTAL_LIMIT]\n",
            "                   [--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
            "                   [--no_cuda [NO_CUDA]] [--seed SEED] [--data_seed DATA_SEED]\n",
            "                   [--bf16 [BF16]] [--fp16 [FP16]]\n",
            "                   [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                   [--half_precision_backend {auto,amp,apex}]\n",
            "                   [--bf16_full_eval [BF16_FULL_EVAL]]\n",
            "                   [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 [TF32]]\n",
            "                   [--local_rank LOCAL_RANK] [--xpu_backend {mpi,ccl}]\n",
            "                   [--tpu_num_cores TPU_NUM_CORES]\n",
            "                   [--tpu_metrics_debug [TPU_METRICS_DEBUG]] [--debug DEBUG]\n",
            "                   [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
            "                   [--eval_steps EVAL_STEPS]\n",
            "                   [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
            "                   [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
            "                   [--disable_tqdm DISABLE_TQDM]\n",
            "                   [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
            "                   [--no_remove_unused_columns]\n",
            "                   [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
            "                   [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
            "                   [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
            "                   [--greater_is_better GREATER_IS_BETTER]\n",
            "                   [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
            "                   [--sharded_ddp SHARDED_DDP] [--deepspeed DEEPSPEED]\n",
            "                   [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
            "                   [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor}]\n",
            "                   [--adafactor [ADAFACTOR]]\n",
            "                   [--group_by_length [GROUP_BY_LENGTH]]\n",
            "                   [--length_column_name LENGTH_COLUMN_NAME]\n",
            "                   [--report_to REPORT_TO [REPORT_TO ...]]\n",
            "                   [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
            "                   [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
            "                   [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
            "                   [--no_dataloader_pin_memory]\n",
            "                   [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
            "                   [--no_skip_memory_metrics]\n",
            "                   [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
            "                   [--push_to_hub [PUSH_TO_HUB]]\n",
            "                   [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "                   [--hub_model_id HUB_MODEL_ID]\n",
            "                   [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
            "                   [--hub_token HUB_TOKEN]\n",
            "                   [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
            "                   [--fp16_backend {auto,amp,apex}]\n",
            "                   [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
            "                   [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
            "                   [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
            "                   [--mp_parameters MP_PARAMETERS]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pretrained model or model identifier from\n",
            "                        huggingface.co/models (default: None)\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name (default: None)\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name (default: None)\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pretrained models\n",
            "                        downloaded from huggingface.co (default: None)\n",
            "  --use_fast_tokenizer [USE_FAST_TOKENIZER]\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not. (default: True)\n",
            "  --no_use_fast_tokenizer\n",
            "                        Whether to use one of the fast tokenizer (backed by\n",
            "                        the tokenizers library) or not. (default: False)\n",
            "  --model_revision MODEL_REVISION\n",
            "                        The specific model version to use (can be a branch\n",
            "                        name, tag name or commit id). (default: main)\n",
            "  --use_auth_token [USE_AUTH_TOKEN]\n",
            "                        Will use the token generated when running\n",
            "                        `transformers-cli login` (necessary to use this script\n",
            "                        with private models). (default: False)\n",
            "  --task_name TASK_NAME\n",
            "                        The name of the task to train on: cola, mnli, mrpc,\n",
            "                        qnli, qqp, rte, sst2, stsb, wnli (default: None)\n",
            "  --dataset_name DATASET_NAME\n",
            "                        The name of the dataset to use (via the datasets\n",
            "                        library). (default: None)\n",
            "  --dataset_config_name DATASET_CONFIG_NAME\n",
            "                        The configuration name of the dataset to use (via the\n",
            "                        datasets library). (default: None)\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded. (default:\n",
            "                        128)\n",
            "  --overwrite_cache [OVERWRITE_CACHE]\n",
            "                        Overwrite the cached preprocessed datasets or not.\n",
            "                        (default: False)\n",
            "  --pad_to_max_length [PAD_TO_MAX_LENGTH]\n",
            "                        Whether to pad all samples to `max_seq_length`. If\n",
            "                        False, will pad the samples dynamically when batching\n",
            "                        to the maximum length in the batch. (default: True)\n",
            "  --no_pad_to_max_length\n",
            "                        Whether to pad all samples to `max_seq_length`. If\n",
            "                        False, will pad the samples dynamically when batching\n",
            "                        to the maximum length in the batch. (default: False)\n",
            "  --max_train_samples MAX_TRAIN_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of training examples to this value if set.\n",
            "                        (default: None)\n",
            "  --max_eval_samples MAX_EVAL_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of evaluation examples to this value if\n",
            "                        set. (default: None)\n",
            "  --max_predict_samples MAX_PREDICT_SAMPLES\n",
            "                        For debugging purposes or quicker training, truncate\n",
            "                        the number of prediction examples to this value if\n",
            "                        set. (default: None)\n",
            "  --train_file TRAIN_FILE\n",
            "                        A csv or a json file containing the training data.\n",
            "                        (default: None)\n",
            "  --validation_file VALIDATION_FILE\n",
            "                        A csv or a json file containing the validation data.\n",
            "                        (default: None)\n",
            "  --test_file TEST_FILE\n",
            "                        A csv or a json file containing the test data.\n",
            "                        (default: None)\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written. (default: None)\n",
            "  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n",
            "                        Overwrite the content of the output directory. Use\n",
            "                        this to continue training if output_dir points to a\n",
            "                        checkpoint directory. (default: False)\n",
            "  --do_train [DO_TRAIN]\n",
            "                        Whether to run training. (default: False)\n",
            "  --do_eval [DO_EVAL]   Whether to run eval on the dev set. (default: False)\n",
            "  --do_predict [DO_PREDICT]\n",
            "                        Whether to run predictions on the test set. (default:\n",
            "                        False)\n",
            "  --evaluation_strategy {no,steps,epoch}\n",
            "                        The evaluation strategy to use. (default: no)\n",
            "  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n",
            "                        When performing evaluation and predictions, only\n",
            "                        returns the loss. (default: False)\n",
            "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for training.\n",
            "                        (default: 8)\n",
            "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
            "                        (default: 8)\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_train_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        training. (default: None)\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
            "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
            "                        evaluation. (default: None)\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass. (default: 1)\n",
            "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
            "                        Number of predictions steps to accumulate before\n",
            "                        moving the tensors to the CPU. (default: None)\n",
            "  --eval_delay EVAL_DELAY\n",
            "                        Number of epochs or steps to wait for before the first\n",
            "                        evaluation can be performed, depending on the\n",
            "                        evaluation_strategy. (default: 0)\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for AdamW. (default: 5e-05)\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay for AdamW if we apply some. (default:\n",
            "                        0.0)\n",
            "  --adam_beta1 ADAM_BETA1\n",
            "                        Beta1 for AdamW optimizer (default: 0.9)\n",
            "  --adam_beta2 ADAM_BETA2\n",
            "                        Beta2 for AdamW optimizer (default: 0.999)\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for AdamW optimizer. (default: 1e-08)\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm. (default: 1.0)\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform. (default:\n",
            "                        3.0)\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs. (default: -1)\n",
            "  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n",
            "                        The scheduler type to use. (default: linear)\n",
            "  --warmup_ratio WARMUP_RATIO\n",
            "                        Linear warmup over warmup_ratio fraction of total\n",
            "                        steps. (default: 0.0)\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps. (default: 0)\n",
            "  --log_level {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on the main node. Possible\n",
            "                        choices are the log levels as strings: 'debug',\n",
            "                        'info', 'warning', 'error' and 'critical', plus a\n",
            "                        'passive' level which doesn't set anything and lets\n",
            "                        the application set the level. Defaults to 'passive'.\n",
            "                        (default: passive)\n",
            "  --log_level_replica {debug,info,warning,error,critical,passive}\n",
            "                        Logger log level to use on replica nodes. Same choices\n",
            "                        and defaults as ``log_level`` (default: passive)\n",
            "  --log_on_each_node [LOG_ON_EACH_NODE]\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "                        (default: True)\n",
            "  --no_log_on_each_node\n",
            "                        When doing a multinode distributed training, whether\n",
            "                        to log once per node or just once on the main node.\n",
            "                        (default: False)\n",
            "  --logging_dir LOGGING_DIR\n",
            "                        Tensorboard log dir. (default: None)\n",
            "  --logging_strategy {no,steps,epoch}\n",
            "                        The logging strategy to use. (default: steps)\n",
            "  --logging_first_step [LOGGING_FIRST_STEP]\n",
            "                        Log the first global_step (default: False)\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps. (default: 500)\n",
            "  --logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]\n",
            "                        Filter nan and inf losses for logging. (default: True)\n",
            "  --no_logging_nan_inf_filter\n",
            "                        Filter nan and inf losses for logging. (default:\n",
            "                        False)\n",
            "  --save_strategy {no,steps,epoch}\n",
            "                        The checkpoint save strategy to use. (default: steps)\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps. (default: 500)\n",
            "  --save_total_limit SAVE_TOTAL_LIMIT\n",
            "                        Limit the total amount of checkpoints. Deletes the\n",
            "                        older checkpoints in the output_dir. Default is\n",
            "                        unlimited checkpoints (default: None)\n",
            "  --save_on_each_node [SAVE_ON_EACH_NODE]\n",
            "                        When doing multi-node distributed training, whether to\n",
            "                        save models and checkpoints on each node, or only on\n",
            "                        the main one (default: False)\n",
            "  --no_cuda [NO_CUDA]   Do not use CUDA even when it is available (default:\n",
            "                        False)\n",
            "  --seed SEED           Random seed that will be set at the beginning of\n",
            "                        training. (default: 42)\n",
            "  --data_seed DATA_SEED\n",
            "                        Random seed to be used with data samplers. (default:\n",
            "                        None)\n",
            "  --bf16 [BF16]         Whether to use bf16 (mixed) precision instead of\n",
            "                        32-bit. Requires Ampere or higher NVIDIA architecture.\n",
            "                        This is an experimental API and it may change.\n",
            "                        (default: False)\n",
            "  --fp16 [FP16]         Whether to use fp16 (mixed) precision instead of\n",
            "                        32-bit (default: False)\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3']. See details at\n",
            "                        https://nvidia.github.io/apex/amp.html (default: O1)\n",
            "  --half_precision_backend {auto,amp,apex}\n",
            "                        The backend to be used for half precision. (default:\n",
            "                        auto)\n",
            "  --bf16_full_eval [BF16_FULL_EVAL]\n",
            "                        Whether to use full bfloat16 evaluation instead of\n",
            "                        32-bit. This is an experimental API and it may change.\n",
            "                        (default: False)\n",
            "  --fp16_full_eval [FP16_FULL_EVAL]\n",
            "                        Whether to use full float16 evaluation instead of\n",
            "                        32-bit (default: False)\n",
            "  --tf32 [TF32]         Whether to enable tf32 mode, available in Ampere and\n",
            "                        newer GPU architectures. This is an experimental API\n",
            "                        and it may change. (default: None)\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank (default: -1)\n",
            "  --xpu_backend {mpi,ccl}\n",
            "                        The backend to be used for distributed training on\n",
            "                        Intel XPU. (default: None)\n",
            "  --tpu_num_cores TPU_NUM_CORES\n",
            "                        TPU: Number of TPU cores (automatically passed by\n",
            "                        launcher script) (default: None)\n",
            "  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n",
            "                        Deprecated, the use of `--debug tpu_metrics_debug` is\n",
            "                        preferred. TPU: Whether to print debug metrics\n",
            "                        (default: False)\n",
            "  --debug DEBUG         Whether or not to enable debug mode. Current options:\n",
            "                        `underflow_overflow` (Detect underflow and overflow in\n",
            "                        activations and weights), `tpu_metrics_debug` (print\n",
            "                        debug metrics on TPU). (default: )\n",
            "  --dataloader_drop_last [DATALOADER_DROP_LAST]\n",
            "                        Drop the last incomplete batch if it is not divisible\n",
            "                        by the batch size. (default: False)\n",
            "  --eval_steps EVAL_STEPS\n",
            "                        Run an evaluation every X steps. (default: None)\n",
            "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
            "                        Number of subprocesses to use for data loading\n",
            "                        (PyTorch only). 0 means that the data will be loaded\n",
            "                        in the main process. (default: 0)\n",
            "  --past_index PAST_INDEX\n",
            "                        If >=0, uses the corresponding part of the output as\n",
            "                        the past state for next step. (default: -1)\n",
            "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
            "                        wandb logging. (default: None)\n",
            "  --disable_tqdm DISABLE_TQDM\n",
            "                        Whether or not to disable the tqdm progress bars.\n",
            "                        (default: None)\n",
            "  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset. (default: True)\n",
            "  --no_remove_unused_columns\n",
            "                        Remove columns not required by the model when using an\n",
            "                        nlp.Dataset. (default: False)\n",
            "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
            "                        The list of keys in your dictionary of inputs that\n",
            "                        correspond to the labels. (default: None)\n",
            "  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n",
            "                        Whether or not to load the best model found during\n",
            "                        training at the end of training. (default: False)\n",
            "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
            "                        The metric to use to compare two different models.\n",
            "                        (default: None)\n",
            "  --greater_is_better GREATER_IS_BETTER\n",
            "                        Whether the `metric_for_best_model` should be\n",
            "                        maximized or not. (default: None)\n",
            "  --ignore_data_skip [IGNORE_DATA_SKIP]\n",
            "                        When resuming training, whether or not to skip the\n",
            "                        first epochs and batches to get to the same training\n",
            "                        data. (default: False)\n",
            "  --sharded_ddp SHARDED_DDP\n",
            "                        Whether or not to use sharded DDP training (in\n",
            "                        distributed training only). The base option should be\n",
            "                        `simple`, `zero_dp_2` or `zero_dp_3` and you can add\n",
            "                        CPU-offload to `zero_dp_2` or `zero_dp_3` like this:\n",
            "                        zero_dp_2 offload` or `zero_dp_3 offload`. You can add\n",
            "                        auto-wrap to `zero_dp_2` or with the same syntax:\n",
            "                        zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`.\n",
            "                        (default: )\n",
            "  --deepspeed DEEPSPEED\n",
            "                        Enable deepspeed and pass the path to deepspeed json\n",
            "                        config file (e.g. ds_config.json) or an already loaded\n",
            "                        json file as a dict (default: None)\n",
            "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
            "                        The label smoothing epsilon to apply (zero means no\n",
            "                        label smoothing). (default: 0.0)\n",
            "  --optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor}\n",
            "                        The optimizer to use. (default: adamw_hf)\n",
            "  --adafactor [ADAFACTOR]\n",
            "                        Whether or not to replace AdamW by Adafactor.\n",
            "                        (default: False)\n",
            "  --group_by_length [GROUP_BY_LENGTH]\n",
            "                        Whether or not to group samples of roughly the same\n",
            "                        length together when batching. (default: False)\n",
            "  --length_column_name LENGTH_COLUMN_NAME\n",
            "                        Column name with precomputed lengths to use when\n",
            "                        grouping by length. (default: length)\n",
            "  --report_to REPORT_TO [REPORT_TO ...]\n",
            "                        The list of integrations to report the results and\n",
            "                        logs to. (default: None)\n",
            "  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `find_unused_parameters` passed to\n",
            "                        `DistributedDataParallel`. (default: None)\n",
            "  --ddp_bucket_cap_mb DDP_BUCKET_CAP_MB\n",
            "                        When using distributed training, the value of the flag\n",
            "                        `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
            "                        (default: None)\n",
            "  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n",
            "                        Whether or not to pin memory for DataLoader. (default:\n",
            "                        True)\n",
            "  --no_dataloader_pin_memory\n",
            "                        Whether or not to pin memory for DataLoader. (default:\n",
            "                        False)\n",
            "  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics. (default: True)\n",
            "  --no_skip_memory_metrics\n",
            "                        Whether or not to skip adding of memory profiler\n",
            "                        reports to metrics. (default: False)\n",
            "  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n",
            "                        Whether or not to use the legacy prediction_loop in\n",
            "                        the Trainer. (default: False)\n",
            "  --push_to_hub [PUSH_TO_HUB]\n",
            "                        Whether or not to upload the trained model to the\n",
            "                        model hub after training. (default: False)\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        The path to a folder with a valid checkpoint for your\n",
            "                        model. (default: None)\n",
            "  --hub_model_id HUB_MODEL_ID\n",
            "                        The name of the repository to keep in sync with the\n",
            "                        local `output_dir`. (default: None)\n",
            "  --hub_strategy {end,every_save,checkpoint,all_checkpoints}\n",
            "                        The hub strategy to use when `--push_to_hub` is\n",
            "                        activated. (default: every_save)\n",
            "  --hub_token HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub. (default:\n",
            "                        None)\n",
            "  --gradient_checkpointing [GRADIENT_CHECKPOINTING]\n",
            "                        If True, use gradient checkpointing to save memory at\n",
            "                        the expense of slower backward pass. (default: False)\n",
            "  --fp16_backend {auto,amp,apex}\n",
            "                        Deprecated. Use half_precision_backend instead\n",
            "                        (default: auto)\n",
            "  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\n",
            "                        The name of the repository to which push the\n",
            "                        `Trainer`. (default: None)\n",
            "  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\n",
            "                        The name of the organization in with to which push the\n",
            "                        `Trainer`. (default: None)\n",
            "  --push_to_hub_token PUSH_TO_HUB_TOKEN\n",
            "                        The token to use to push to the Model Hub. (default:\n",
            "                        None)\n",
            "  --mp_parameters MP_PARAMETERS\n",
            "                        Used by the SageMaker launcher to send mp-specific\n",
            "                        args. Ignored in Trainer (default: )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIMBhYPcqV4h"
      },
      "outputs": [],
      "source": [
        "# ! python3 run_glue.py --help/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAIl9itTThe_",
        "outputId": "ee033b6c-f72b-49df-835b-1e847d10699f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 1043.01it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-04-06 20:34:45,337 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-04-06 20:34:45,338 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-04-06 20:34:45,878 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-04-06 20:34:45,879 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-06 20:34:47,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-06 20:34:47,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-06 20:34:47,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-06 20:34:47,509 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1778] 2022-04-06 20:34:47,510 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:654] 2022-04-06 20:34:47,780 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:690] 2022-04-06 20:34:47,780 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1772] 2022-04-06 20:34:48,114 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:2049] 2022-04-06 20:34:49,553 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2060] 2022-04-06 20:34:49,553 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  5.66ba/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 62.60ba/s]\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00, 67.90ba/s]\n",
            "[INFO|trainer.py:567] 2022-04-06 20:34:53,868 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1290] 2022-04-06 20:34:53,883 >> ***** Running training *****\n",
            "[INFO|trainer.py:1291] 2022-04-06 20:34:53,883 >>   Num examples = 393\n",
            "[INFO|trainer.py:1292] 2022-04-06 20:34:53,883 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1293] 2022-04-06 20:34:53,883 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1294] 2022-04-06 20:34:53,883 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1295] 2022-04-06 20:34:53,883 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1296] 2022-04-06 20:34:53,883 >>   Total optimization steps = 125\n",
            "100% 125/125 [01:45<00:00,  1.33it/s][INFO|trainer.py:1530] 2022-04-06 20:36:39,790 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 125/125 [01:45<00:00,  1.18it/s]\n",
            "[INFO|trainer.py:2166] 2022-04-06 20:36:39,799 >> Saving model checkpoint to /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out\n",
            "[INFO|configuration_utils.py:441] 2022-04-06 20:36:39,807 >> Configuration saved in /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out/config.json\n",
            "[INFO|modeling_utils.py:1378] 2022-04-06 20:36:41,285 >> Model weights saved in /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:36:41,290 >> tokenizer config file saved in /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:36:41,294 >> Special tokens file saved in /content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out/special_tokens_map.json\n",
            "[INFO|trainer.py:567] 2022-04-06 20:36:41,988 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-04-06 20:36:41,991 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2418] 2022-04-06 20:36:41,991 >>   Num examples = 50\n",
            "[INFO|trainer.py:2421] 2022-04-06 20:36:41,991 >>   Batch size = 16\n",
            "100% 4/4 [00:00<00:00,  6.00it/s]\n",
            "[INFO|trainer.py:567] 2022-04-06 20:36:42,970 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2416] 2022-04-06 20:36:42,973 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2418] 2022-04-06 20:36:42,973 >>   Num examples = 49\n",
            "[INFO|trainer.py:2421] 2022-04-06 20:36:42,973 >>   Batch size = 16\n",
            " 75% 3/4 [00:00<00:00,  4.69it/s][INFO|modelcard.py:460] 2022-04-06 20:36:44,195 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'F1', 'type': 'f1', 'value': 0.8599999999999999}]}\n",
            "100% 4/4 [00:00<00:00,  4.32it/s]\n"
          ]
        }
      ],
      "source": [
        "! python run_glue.py \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir \\\n",
        "  --max_seq_length 512 \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --per_device_eval_batch_size 16 \\\n",
        "  --num_train_epochs 5 \\\n",
        "  --output_dir \"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out\" \\\n",
        "  --test_file \"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/test_tweets_text_userdescriptions.csv\" \\\n",
        "  --train_file \"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/train_tweets_text_userdescriptions.csv\" \\\n",
        "  --validation_file  \"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/val_tweets_text_userdescriptions.csv\" \\\n",
        "  --seed 1 \\\n",
        "  > out.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fmVMVyMN-TS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "with open(\"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/out/predict_results_None.txt\", \"r\") as f:\n",
        "\tlines = f.readlines()\n",
        "\tfor line in lines:\n",
        "\t\tline = line.strip()\n",
        "\t\tline = line.split(\"\\t\")\n",
        "\t\tprint(line)\n",
        "\t\tprint(line[0], line[1])\n",
        "\t\t# if the prediction is not a number skip it\n",
        "\t\ttry:\n",
        "\t\t\tpredictions.append(int(line[1]))\n",
        "\t\texcept:\n",
        "\t\t\tcontinue\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMamPmj1-QzC",
        "outputId": "bebfdfa5-d117-4e76-ff23-df0640471ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['index', 'prediction']\n",
            "index prediction\n",
            "['0', '1']\n",
            "0 1\n",
            "['1', '0']\n",
            "1 0\n",
            "['2', '0']\n",
            "2 0\n",
            "['3', '0']\n",
            "3 0\n",
            "['4', '1']\n",
            "4 1\n",
            "['5', '0']\n",
            "5 0\n",
            "['6', '0']\n",
            "6 0\n",
            "['7', '1']\n",
            "7 1\n",
            "['8', '0']\n",
            "8 0\n",
            "['9', '0']\n",
            "9 0\n",
            "['10', '0']\n",
            "10 0\n",
            "['11', '0']\n",
            "11 0\n",
            "['12', '0']\n",
            "12 0\n",
            "['13', '1']\n",
            "13 1\n",
            "['14', '1']\n",
            "14 1\n",
            "['15', '1']\n",
            "15 1\n",
            "['16', '0']\n",
            "16 0\n",
            "['17', '0']\n",
            "17 0\n",
            "['18', '0']\n",
            "18 0\n",
            "['19', '1']\n",
            "19 1\n",
            "['20', '1']\n",
            "20 1\n",
            "['21', '0']\n",
            "21 0\n",
            "['22', '0']\n",
            "22 0\n",
            "['23', '1']\n",
            "23 1\n",
            "['24', '0']\n",
            "24 0\n",
            "['25', '1']\n",
            "25 1\n",
            "['26', '1']\n",
            "26 1\n",
            "['27', '0']\n",
            "27 0\n",
            "['28', '1']\n",
            "28 1\n",
            "['29', '0']\n",
            "29 0\n",
            "['30', '0']\n",
            "30 0\n",
            "['31', '0']\n",
            "31 0\n",
            "['32', '1']\n",
            "32 1\n",
            "['33', '1']\n",
            "33 1\n",
            "['34', '0']\n",
            "34 0\n",
            "['35', '0']\n",
            "35 0\n",
            "['36', '1']\n",
            "36 1\n",
            "['37', '0']\n",
            "37 0\n",
            "['38', '1']\n",
            "38 1\n",
            "['39', '0']\n",
            "39 0\n",
            "['40', '0']\n",
            "40 0\n",
            "['41', '0']\n",
            "41 0\n",
            "['42', '0']\n",
            "42 0\n",
            "['43', '0']\n",
            "43 0\n",
            "['44', '1']\n",
            "44 1\n",
            "['45', '0']\n",
            "45 0\n",
            "['46', '0']\n",
            "46 0\n",
            "['47', '1']\n",
            "47 1\n",
            "['48', '1']\n",
            "48 1\n",
            "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels_df = pd.read_csv(\"/content/drive/MyDrive/WPI/Junior/IQP/Machine Models/data/huggingface_v3/user/test_tweets_text_userdescriptions.csv\")\n",
        "true_labels = true_labels_df[\"label\"].tolist()\n",
        "print(true_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvoYdEu_-lmD",
        "outputId": "c5d12a27-3b35-4fe2-e6a4-7cce29ccd1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sklearns classification report to get metrics\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(true_labels, predictions)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sas7sxVa-nbH",
        "outputId": "fdd5f703-4556-476d-8c8b-ae9e4374a1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97        32\n",
            "           1       0.89      1.00      0.94        17\n",
            "\n",
            "    accuracy                           0.96        49\n",
            "   macro avg       0.95      0.97      0.96        49\n",
            "weighted avg       0.96      0.96      0.96        49\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Huggingface_train_tenfold.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}